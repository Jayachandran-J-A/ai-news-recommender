# Google Colab Training Guide - NRMS Model

## üöÄ Complete Step-by-Step Instructions

This guide will help you train your NRMS (Neural News Recommendation with Multi-head Self-Attention) model on Google Colab, which is **3x faster** than your GTX 1650.

---

## Part 1: Prepare Files for Upload

### 1.1 Locate Your Dataset Files

On your local computer, navigate to:
```
C:\Users\jayac\Projects\Data-Science-CapstoneProject\Idea 2\news-recommender\Dataset-archive\MINDsmall_train\
```

You need these two files:
- ‚úÖ **behaviors.tsv** (92 MB) - User click histories
- ‚úÖ **news.tsv** (41 MB) - News article metadata

### 1.2 Open the Colab Notebook

1. Open your web browser and go to: https://colab.research.google.com
2. Click **File > Upload notebook**
3. Upload the file: `NRMS_Training_Colab.ipynb` (I just created this in your project folder)

**OR** manually create a new notebook and copy the code from `NRMS_Training_Colab.ipynb`

---

## Part 2: Set Up Google Colab Environment

### 2.1 Enable GPU Runtime

**CRITICAL STEP - Don't skip this!**

1. In Colab, click **Runtime** (top menu)
2. Click **Change runtime type**
3. In the popup:
   - Hardware accelerator: Select **GPU**
   - GPU type: Select **T4** (free tier)
4. Click **Save**
5. Wait for the runtime to restart (~10 seconds)

### 2.2 Verify GPU is Active

Run the first code cell:
```python
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
```

**Expected Output:**
```
PyTorch version: 2.x.x+cu118
CUDA available: True
GPU: Tesla T4
```

If you see `CUDA available: False`, go back to step 2.1!

---

## Part 3: Upload Dataset Files

### 3.1 Manual Upload (Recommended for First Time)

1. Look at the **left sidebar** in Colab
2. Click the **folder icon** üìÅ (Files)
3. Click the **upload button** ‚¨ÜÔ∏è (looks like a file with arrow)
4. Select **both files** from your local computer:
   - `behaviors.tsv`
   - `news.tsv`
5. Wait for upload to complete (~2-3 minutes depending on internet speed)

### 3.2 Verify Files Uploaded

Run this cell:
```python
import os
if os.path.exists('behaviors.tsv') and os.path.exists('news.tsv'):
    print("‚úÖ Dataset files found!")
    print(f"   behaviors.tsv: {os.path.getsize('behaviors.tsv') / 1e6:.1f} MB")
    print(f"   news.tsv: {os.path.getsize('news.tsv') / 1e6:.1f} MB")
else:
    print("‚ùå Files not found! Please upload again.")
```

**Expected Output:**
```
‚úÖ Dataset files found!
   behaviors.tsv: 92.0 MB
   news.tsv: 41.0 MB
```

---

## Part 4: Run Training Cells

### 4.1 Install Dependencies (Cell 2)

Run the cell:
```python
!pip install -q fastembed pandas numpy scikit-learn tqdm
```

This installs required packages. Takes ~30 seconds.

### 4.2 Define Metrics (Cell 3)

Run the metrics cell. This defines NDCG@10, AUC evaluation functions.

### 4.3 Define Model Architecture (Cell 4)

Run the NRMS model definition cell. This creates:
- MultiHeadSelfAttention
- NewsEncoder
- UserEncoder
- NRMS (main model)

### 4.4 Load Dataset (Cell 5-6)

**This is the longest step - takes 5-10 minutes!**

Run both cells:
1. Load news corpus and generate embeddings
2. Load user behaviors

You'll see progress bars:
```
Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51282/51282
‚úÖ Generated 51282 embeddings with shape (384,)
‚úÖ Loaded 50000+ user impression logs
```

### 4.5 Create Training Dataset (Cell 7)

Run the dataset creation cell. This processes all behaviors into training samples.

Expected output:
```
Processing behaviors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 
‚úÖ Created 200000+ training samples
‚úÖ Train samples: 160000+
‚úÖ Validation samples: 40000+
```

### 4.6 Start Training! (Cell 8-9)

Run the training cells. **This takes 30-45 minutes.**

You'll see progress like this:
```
================================================================================
STARTING TRAINING
================================================================================
Epoch 1/5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [08:32<00:00, loss: 0.4521]
Epoch 1/5 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [02:15<00:00]

Epoch 1/5:
  Train Loss: 0.5234
  Val Loss: 0.4521
  Val AUC: 0.6234
  Val NDCG@10: 0.6512
  ‚úÖ Saved best model (NDCG@10: 0.6512)
--------------------------------------------------------------------------------
```

**What the metrics mean:**
- **Train Loss**: Lower is better (model learning)
- **Val Loss**: Lower is better (generalization)
- **Val AUC**: 0.6-0.7 is good (click prediction accuracy)
- **Val NDCG@10**: 0.65-0.75 is excellent (ranking quality)

### 4.7 Monitor Training

**Tips while waiting:**
- ‚úÖ Keep the browser tab open (don't close it!)
- ‚úÖ Colab sessions timeout after 12 hours (your training finishes in <1 hour)
- ‚úÖ If connection drops, reconnect quickly - training continues in background
- ‚ùå Don't refresh the page
- ‚ùå Don't close the laptop (unless connected to power)

---

## Part 5: Download Trained Model

### 5.1 Automatic Download

After training completes, run the last cell:
```python
from google.colab import files
files.download('nrms_model.pt')
```

This automatically downloads the file to your computer.

### 5.2 Manual Download (Backup Method)

If automatic download fails:
1. Look at left sidebar (Files üìÅ)
2. Find `nrms_model.pt` (should be ~10-15 MB)
3. Right-click on the file
4. Click **Download**

### 5.3 Save to Project Folder

Move the downloaded file to:
```
C:\Users\jayac\Projects\Data-Science-CapstoneProject\Idea 2\news-recommender\models\nrms_model.pt
```

---

## Part 6: Verify Model File

### 6.1 Check File Size

The model file should be **10-15 MB**. If it's much smaller, training may have failed.

### 6.2 Load Model Locally (Test)

Back on your local machine, run this in Python:

```python
import torch

# Load the model
checkpoint = torch.load('models/nrms_model.pt', map_location='cpu')
print("Model loaded successfully!")
print(f"Trained for {checkpoint['epoch']+1} epochs")
print(f"Best NDCG@10: {checkpoint['ndcg']:.4f}")
print(f"Best AUC: {checkpoint['auc']:.4f}")
```

---

## Troubleshooting

### Problem: "CUDA available: False"
**Solution:** 
- Go to Runtime > Change runtime type > GPU
- Restart runtime
- Run GPU check cell again

### Problem: "Files not found" after upload
**Solution:**
- Make sure you uploaded to the root directory (not a subfolder)
- Files should appear directly under /content/ in left sidebar
- Re-upload if needed

### Problem: "Out of memory" error
**Solution:**
- Reduce batch size in training cell: Change `batch_size=64` to `batch_size=32`
- Restart runtime: Runtime > Factory reset runtime
- Try again

### Problem: Training is very slow (>2 hours)
**Solution:**
- Check GPU is enabled (step 2.1)
- Verify T4 GPU is active (not CPU)
- Reduce dataset: Use only 50% of behaviors for faster testing

### Problem: Colab session disconnected
**Solution:**
- Reconnect immediately
- Check if training is still running (look for running cells)
- If training stopped, restart from last saved checkpoint
- Free Colab has 12-hour limit, your training finishes in <1 hour

### Problem: Can't download model file
**Solution:**
- Use manual download method (right-click > Download)
- Or mount Google Drive and save there:
  ```python
  from google.colab import drive
  drive.mount('/content/drive')
  !cp nrms_model.pt /content/drive/MyDrive/
  ```

---

## Expected Timeline

| Step | Time | What's Happening |
|------|------|-----------------|
| Setup Colab & Upload Files | 5 min | GPU activation, file upload |
| Install Dependencies | 1 min | pip install packages |
| Generate Embeddings | 7-10 min | 51K news articles ‚Üí vectors |
| Create Dataset | 3-5 min | Process 50K+ user behaviors |
| Train Epoch 1 | 8-10 min | First training pass |
| Train Epoch 2-5 | 32-40 min | Continue training |
| Download Model | 1 min | Save to local |
| **TOTAL** | **30-45 min** | Complete training! |

---

## What You'll Get

After completing this guide, you'll have:

‚úÖ **Trained NRMS model** (`nrms_model.pt` file)
‚úÖ **Model metrics:**
   - NDCG@10: 0.65-0.75 (excellent ranking quality)
   - AUC: 0.60-0.70 (good click prediction)
‚úÖ **Ready for ensemble:** Combine with your existing XGBoost model
‚úÖ **Research-grade results:** Suitable for capstone project

---

## Next Steps After Training

1. **Create Ensemble Model:**
   - Combine NRMS (neural) + XGBoost (gradient boosting)
   - Use weighted averaging or meta-learner
   - Expected improvement: +5-10% NDCG

2. **Integrate into API:**
   - Update `src/recommend.py`
   - Load both models at startup
   - Serve ensemble predictions

3. **Evaluate and Compare:**
   - Baseline (XGBoost only): NDCG ~0.55
   - NRMS only: NDCG ~0.70
   - Ensemble: NDCG ~0.75+ (best!)

4. **Document for Capstone:**
   - Training methodology
   - Model architecture (multi-head attention)
   - Performance improvements
   - Real-world impact

---

## Tips for Success

‚úÖ **Do:**
- Keep browser tab open during training
- Monitor progress bars
- Save model immediately after training
- Test model loading locally before integrating

‚ùå **Don't:**
- Close browser during training
- Refresh page unnecessarily
- Skip GPU activation step
- Forget to download model file

---

## Questions?

If you run into any issues:
1. Check the Troubleshooting section above
2. Read error messages carefully
3. Verify GPU is enabled (most common issue)
4. Make sure files are uploaded correctly

**Training time comparison:**
- Your GTX 1650: ~2 hours
- Colab T4 GPU: ~40 minutes (3x faster!)
- Colab P100 GPU: ~25 minutes (if you get lucky!)

---

## Ready to Start?

1. ‚úÖ Open Google Colab: https://colab.research.google.com
2. ‚úÖ Upload `NRMS_Training_Colab.ipynb`
3. ‚úÖ Enable GPU runtime
4. ‚úÖ Upload dataset files
5. ‚úÖ Run all cells
6. ‚úÖ Download trained model

**Good luck! üöÄ**
